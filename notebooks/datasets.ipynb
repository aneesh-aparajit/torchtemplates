{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495560da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff8d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    '''Dataset class for text data from tabular data for Classification'''\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: str, pretrained_weights: str) -> None:\n",
    "        '''Constructor for TextDatasetForClassification\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        - df: pd.DataFrame\n",
    "            - The dataframe containing the required columns.\n",
    "        - tokenizer: str\n",
    "            - The name of the tokenizer you want to use\n",
    "        - pretrained_weights: str\n",
    "            - The pretrained weights you want to use\n",
    "        '''\n",
    "        super(TextDataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = getattr(transformers, tokenizer)\n",
    "        self.tokenizer = self.tokenizer.from_pretrained(pretrained_weights)\n",
    "        # Write extra logic if needed\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, ix: int) -> Dict[str, torch.Tensor]:\n",
    "        # Implement the __getitem__ logic here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "609a2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2ImageDataset(Dataset):\n",
    "    '''Dataset class for image relate tasks'''\n",
    "    def __init__(self, input_paths: List[str], output_paths: List[str], mode: str) -> None:\n",
    "        super(Image2ImageDataset, self).__init__()\n",
    "        self.input_paths = input_paths\n",
    "        self.output_paths = output_paths\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            self.transforms = A.Compose([\n",
    "                A.Resize(height=config.image_size[0], width=config.image_size[1], interpolation=0),\n",
    "                A.Rotate(limit=90, p=0.5),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.ColorJitter(p=0.5),\n",
    "                A.Normalize(),\n",
    "                ToTensorV2()\n",
    "            ], p=1.0)\n",
    "        elif self.mode == \"valid\":\n",
    "            self.transforms = A.Compose([\n",
    "                A.Resize(height=config.image_size[0], width=config.image_size[1], interpolation=0),\n",
    "                A.Normalize(),\n",
    "                ToTensorV2()\n",
    "            ], p=1.0)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"The case where mode={self.mode} has not been implemented try from ['train', 'valid']\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_paths)\n",
    "    \n",
    "    def __getitem__(self, ix: int) -> Dict[str, torch.Tensor]:\n",
    "        input_path = self.input_paths[ix]\n",
    "        output_path = self.output_paths[ix]\n",
    "        \n",
    "        input_img = np.array(Image.open(input_path))\n",
    "        output_img = np.array(Image.open(output_path))\n",
    "        \n",
    "        transformed = self.transforms(image=input_img, mask=output_img)\n",
    "        \n",
    "        image = torch.tensor(transformed['image'], dtype=torch.float32, device=config.device)\n",
    "        mask = torch.tensor(transformed['mask'], dtype=torch.float32, device=config.device)\n",
    "        mask = mask.permute(2, 0, 1)\n",
    "        return {\n",
    "            \"image\": image / 255.0,\n",
    "            \"mask\": mask / 255.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20cedf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelName(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ModelName, self).__init__()\n",
    "        # Write Model information here...\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Implement forward \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2eba6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5d4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        super(TabularDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, ix: int) -> Dict[str, torch.Tensor]:\n",
    "        X = self.X[ix]\n",
    "        y = self.y[ix]\n",
    "        \n",
    "        return {\n",
    "            'X': torch.Tensor(X, dtype=torch.float32),\n",
    "            'y': torch.Tensor(y, dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1524d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "def train_one_step(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim,\n",
    "    scheduler: lr_scheduler, \n",
    "    loader: DataLoader, \n",
    "    epoch: int,\n",
    "    criterion\n",
    ") -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    dataset_size = 0.0\n",
    "    total_size   = len(loader)\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=f'Epoch [{epoch}] (train) ')\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "        X, y = batch['X'], batch['y'] # change as per need\n",
    "        bs = X.shape[0]\n",
    "        \n",
    "        yHat = model.forward(X)\n",
    "        loss = criterion(yHat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        running_loss += (loss.item() * bs)\n",
    "        dataset_size += bs\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            epoch_loss=f'{epoch_loss:.5f}',\n",
    "            current_lr=f'{current_lr:.5f}'\n",
    "        )\n",
    "        \n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37c1f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_step(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim,\n",
    "    scheduler: lr_scheduler, \n",
    "    loader: DataLoader, \n",
    "    epoch: int,\n",
    "    criterion\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    dataset_size = 0.0\n",
    "    total_size   = len(loader)\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=f'Epoch [{epoch}] (valid) ')\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "        X, y = batch['X'], batch['y'] # change as per need\n",
    "        bs = X.shape[0]\n",
    "        \n",
    "        yHat = model.forward(X)\n",
    "        loss = criterion(yHat, y)\n",
    "            \n",
    "        running_loss += (loss.item() * bs)\n",
    "        dataset_size += bs\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            epoch_loss=f'{epoch_loss:.5f}'\n",
    "        )\n",
    "        \n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f798efd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (107942307.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    # Write Code...\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def run_training(fold: int):\n",
    "    # Write Code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class config:\n",
    "    seed          = 42\n",
    "    exp_name      = None\n",
    "    model_name    = None\n",
    "    base_model    = None\n",
    "    train_bs      = 32\n",
    "    valid_bs      = 2 * train_bs\n",
    "    image_size    = [224, 224]\n",
    "    in_channels   = 3\n",
    "    latent_size   = 128\n",
    "    hidden_size   = 512\n",
    "    num_layers    = 2\n",
    "    bidirectional = 0 # could be 0 or 1\n",
    "    comment       = None\n",
    "    epochs        = 10\n",
    "    learning_rate = 3e-4\n",
    "    scheduler     = 'CosineAnnealingLR'\n",
    "    min_lr        = 1e-6\n",
    "    T_max         = int(30000/train_bs*epochs)+50\n",
    "    T_0           = 25\n",
    "    warmup_epochs = 0\n",
    "    wd            = 1e-6\n",
    "    n_accumulate  = max(1, 32//train_bs)\n",
    "    n_fold        = 5\n",
    "    num_classes   = 3\n",
    "    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "    train_num     = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
